{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(edge_index=[2, 734640], text_nodes=[216613], text_edges=[734640], text_node_labels=[216613], edge_score=[734640])\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# 定义新文件路径\n",
    "goodreads_children_path = 'Dataset/goodreads_children/processed/children.pkl'\n",
    "\n",
    "# 加载 goodreads_children.pkl 文件\n",
    "with open(goodreads_children_path, 'rb') as f:\n",
    "    goodreads_children_data = pickle.load(f)\n",
    "\n",
    "# 打印加载的数据\n",
    "print(goodreads_children_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_labels = goodreads_children_data.text_node_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m mlb \u001b[38;5;241m=\u001b[39m MultiLabelBinarizer()\n\u001b[0;32m----> 2\u001b[0m binary_labels \u001b[38;5;241m=\u001b[39m \u001b[43mmlb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproduct_labels\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/preprocessing/_label.py:819\u001b[0m, in \u001b[0;36mMultiLabelBinarizer.fit_transform\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    817\u001b[0m class_mapping \u001b[38;5;241m=\u001b[39m defaultdict(\u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m    818\u001b[0m class_mapping\u001b[38;5;241m.\u001b[39mdefault_factory \u001b[38;5;241m=\u001b[39m class_mapping\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__len__\u001b[39m\n\u001b[0;32m--> 819\u001b[0m yt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_mapping\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    821\u001b[0m \u001b[38;5;66;03m# sort classes and reorder columns\u001b[39;00m\n\u001b[1;32m    822\u001b[0m tmp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(class_mapping, key\u001b[38;5;241m=\u001b[39mclass_mapping\u001b[38;5;241m.\u001b[39mget)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/preprocessing/_label.py:892\u001b[0m, in \u001b[0;36mMultiLabelBinarizer._transform\u001b[0;34m(self, y, class_mapping)\u001b[0m\n\u001b[1;32m    890\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m labels \u001b[38;5;129;01min\u001b[39;00m y:\n\u001b[1;32m    891\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[0;32m--> 892\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m labels:\n\u001b[1;32m    893\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    894\u001b[0m             index\u001b[38;5;241m.\u001b[39madd(class_mapping[label])\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "binary_labels = mlb.fit_transform(product_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "dgl_graph = goodreads_children_data\n",
    "test_ratio=0.1\n",
    "val_ratio=0.2\n",
    "random_seed=42\n",
    "neg_len=\"1000\"\n",
    "path=None\n",
    "way=\"random\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lizhuofeng/.local/lib/python3.10/site-packages/torch_geometric/data/storage.py:452: UserWarning: Unable to accurately infer 'num_nodes' from the attribute set '{'edge_index', 'text_edges', 'text_nodes'}'. Please explicitly set 'num_nodes' as an attribute of 'data' to suppress this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "graph = dgl_graph\n",
    "\n",
    "eids = np.arange(graph.num_edges)\n",
    "eids = np.random.permutation(eids)\n",
    "\n",
    "u, v = graph.edge_index\n",
    "\n",
    "test_size = int(len(eids) * test_ratio)\n",
    "val_size = int(len(eids) * val_ratio)\n",
    "\n",
    "test_pos_u, test_pos_v = u[eids[:test_size]], v[eids[:test_size]]\n",
    "val_pos_u, val_pos_v = (\n",
    "\tu[eids[test_size : test_size + val_size]],\n",
    "\tv[eids[test_size : test_size + val_size]],\n",
    ")\n",
    "train_pos_u, train_pos_v = (\n",
    "\tu[eids[test_size + val_size :]],\n",
    "\tv[eids[test_size + val_size :]],\n",
    ")\n",
    "\n",
    "valid_target_neg = th.randint(\n",
    "\t0, graph.num_nodes, [len(val_pos_u), int(neg_len)], dtype=th.long\n",
    ")  # TODO: filter some edges in graph\n",
    "test_target_neg = th.randint(\n",
    "\t0, graph.num_nodes, [len(test_pos_u), int(neg_len)], dtype=th.long\n",
    ")  # TODO: filter some edges in graph\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_sparse import SparseTensor\n",
    "\n",
    "\n",
    "adj_t = SparseTensor.from_edge_index(goodreads_children_data.edge_index, sparse_sizes=(goodreads_children_data.num_nodes, goodreads_children_data.num_nodes)).t()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lizhuofeng/.local/lib/python3.10/site-packages/torch_geometric/data/storage.py:452: UserWarning: Unable to accurately infer 'num_nodes' from the attribute set '{'edge_index', 'text_edges', 'text_nodes'}'. Please explicitly set 'num_nodes' as an attribute of 'data' to suppress this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "60785"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "goodreads_children_data.num_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60785, 60785)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj_t.size(0), adj_t.size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4, 9])\n",
      "tensor([6, 8])\n",
      "tensor([0, 5])\n",
      "tensor([7, 3])\n",
      "tensor([1, 2])\n"
     ]
    }
   ],
   "source": [
    "for perm in DataLoader(range(10), 2, shuffle=True):\n",
    "    print(perm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://data.dgl.ai/dataset/reddit.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting ./raw/reddit.zip\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import os.path as osp\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch_geometric.datasets import Reddit\n",
    "from torch_geometric.loader import NeighborLoader\n",
    "from torch_geometric.nn import SAGEConv\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "dataset = Reddit(\"Reddit\")\n",
    "\n",
    "# Already send node features/labels to GPU for faster access during sampling:\n",
    "data = dataset[0].to(device, 'x', 'y')\n",
    "\n",
    "kwargs = {'batch_size': 1024, 'num_workers': 6, 'persistent_workers': True}\n",
    "train_loader = NeighborLoader(data, input_nodes=data.train_mask,\n",
    "                              num_neighbors=[25, 10], shuffle=True, **kwargs)\n",
    "\n",
    "subgraph_loader = NeighborLoader(copy.copy(data), input_nodes=None,\n",
    "                                 num_neighbors=[-1], shuffle=False, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([106735]) torch.Size([106735, 602]) 1024\n",
      "torch.Size([105481]) torch.Size([105481, 602]) 1024\n",
      "torch.Size([105129]) torch.Size([105129, 602]) 1024\n",
      "torch.Size([106220]) torch.Size([106220, 602]) 1024\n",
      "torch.Size([105261]) torch.Size([105261, 602]) 1024\n",
      "torch.Size([106320]) torch.Size([106320, 602]) 1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([105405]) torch.Size([105405, 602]) 1024\n",
      "torch.Size([106596]) torch.Size([106596, 602]) 1024\n",
      "torch.Size([105676]) torch.Size([105676, 602]) 1024\n",
      "torch.Size([104610]) torch.Size([104610, 602]) 1024\n",
      "torch.Size([106998]) torch.Size([106998, 602]) 1024\n",
      "torch.Size([106631]) torch.Size([106631, 602]) 1024\n",
      "torch.Size([105737]) torch.Size([105737, 602]) 1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([105855]) torch.Size([105855, 602]) 1024\n",
      "torch.Size([105222]) torch.Size([105222, 602]) 1024\n",
      "torch.Size([105959]) torch.Size([105959, 602]) 1024\n",
      "torch.Size([105581]) torch.Size([105581, 602]) 1024\n",
      "torch.Size([106453]) torch.Size([106453, 602]) 1024\n",
      "torch.Size([105209]) torch.Size([105209, 602]) 1024\n",
      "torch.Size([105987]) torch.Size([105987, 602]) 1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([105805]) torch.Size([105805, 602]) 1024\n",
      "torch.Size([105909]) torch.Size([105909, 602]) 1024\n",
      "torch.Size([105238]) torch.Size([105238, 602]) 1024\n",
      "torch.Size([105930]) torch.Size([105930, 602]) 1024\n",
      "torch.Size([105840]) torch.Size([105840, 602]) 1024\n",
      "torch.Size([105641]) torch.Size([105641, 602]) 1024\n",
      "torch.Size([105828]) torch.Size([105828, 602]) 1024\n",
      "torch.Size([105311]) torch.Size([105311, 602]) 1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([105526]) torch.Size([105526, 602]) 1024\n",
      "torch.Size([105432]) torch.Size([105432, 602]) 1024\n",
      "torch.Size([105646]) torch.Size([105646, 602]) 1024\n",
      "torch.Size([105406]) torch.Size([105406, 602]) 1024\n",
      "torch.Size([105674]) torch.Size([105674, 602]) 1024\n",
      "torch.Size([105651]) torch.Size([105651, 602]) 1024\n",
      "torch.Size([106277]) torch.Size([106277, 602]) 1024\n",
      "torch.Size([105943]) torch.Size([105943, 602]) 1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([105412]) torch.Size([105412, 602]) 1024\n",
      "torch.Size([105830]) torch.Size([105830, 602]) 1024\n",
      "torch.Size([105797]) torch.Size([105797, 602]) 1024\n",
      "torch.Size([104492]) torch.Size([104492, 602]) 1024\n",
      "torch.Size([106243]) torch.Size([106243, 602]) 1024\n",
      "torch.Size([105881]) torch.Size([105881, 602]) 1024\n",
      "torch.Size([104997]) torch.Size([104997, 602]) 1024\n",
      "torch.Size([105985]) torch.Size([105985, 602]) 1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([106930]) torch.Size([106930, 602]) 1024\n",
      "torch.Size([105929]) torch.Size([105929, 602]) 1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  85%|████████▍ | 395781/465930 [01:48<00:19, 3662.48it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 116\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m11\u001b[39m):\n\u001b[1;32m    115\u001b[0m     start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 116\u001b[0m     loss, acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m02d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Approx. Train: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00macc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    118\u001b[0m     train_acc, val_acc, test_acc \u001b[38;5;241m=\u001b[39m test()\n",
      "Cell \u001b[0;32mIn[17], line 89\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     87\u001b[0m y_hat \u001b[38;5;241m=\u001b[39m model(batch\u001b[38;5;241m.\u001b[39mx, batch\u001b[38;5;241m.\u001b[39medge_index\u001b[38;5;241m.\u001b[39mto(device))[:batch\u001b[38;5;241m.\u001b[39mbatch_size]\n\u001b[1;32m     88\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(y_hat, y)\n\u001b[0;32m---> 89\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     92\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(loss) \u001b[38;5;241m*\u001b[39m batch\u001b[38;5;241m.\u001b[39mbatch_size\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import os.path as osp\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch_geometric.datasets import Reddit\n",
    "from torch_geometric.loader import NeighborLoader\n",
    "from torch_geometric.nn import SAGEConv\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# path = osp.join(osp.dirname(osp.realpath(__file__)), '..', 'data', 'Reddit')\n",
    "dataset = Reddit(\"Reddit\")\n",
    "\n",
    "# Already send node features/labels to GPU for faster access during sampling:\n",
    "data = dataset[0].to(device, 'x', 'y')\n",
    "\n",
    "kwargs = {'batch_size': 1024, 'num_workers': 6, 'persistent_workers': True}\n",
    "train_loader = NeighborLoader(data, input_nodes=data.train_mask,\n",
    "                              num_neighbors=[25, 10], shuffle=True, **kwargs)\n",
    "\n",
    "subgraph_loader = NeighborLoader(copy.copy(data), input_nodes=None,\n",
    "                                 num_neighbors=[-1], shuffle=False, **kwargs)\n",
    "\n",
    "# No need to maintain these features during evaluation:\n",
    "del subgraph_loader.data.x, subgraph_loader.data.y\n",
    "# Add global node index information.\n",
    "subgraph_loader.data.num_nodes = data.num_nodes\n",
    "subgraph_loader.data.n_id = torch.arange(data.num_nodes)\n",
    "\n",
    "\n",
    "class SAGE(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.convs.append(SAGEConv(in_channels, hidden_channels))\n",
    "        self.convs.append(SAGEConv(hidden_channels, out_channels))\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            x = conv(x, edge_index)\n",
    "            if i < len(self.convs) - 1:\n",
    "                x = x.relu_()\n",
    "                x = F.dropout(x, p=0.5, training=self.training)\n",
    "        return x\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def inference(self, x_all, subgraph_loader):\n",
    "        pbar = tqdm(total=len(subgraph_loader.dataset) * len(self.convs))\n",
    "        pbar.set_description('Evaluating')\n",
    "\n",
    "        # Compute representations of nodes layer by layer, using *all*\n",
    "        # available edges. This leads to faster computation in contrast to\n",
    "        # immediately computing the final representations of each batch:\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            xs = []\n",
    "            for batch in subgraph_loader:\n",
    "                x = x_all[batch.n_id.to(x_all.device)].to(device)\n",
    "                x = conv(x, batch.edge_index.to(device))\n",
    "                if i < len(self.convs) - 1:\n",
    "                    x = x.relu_()\n",
    "                xs.append(x[:batch.batch_size].cpu())\n",
    "                pbar.update(batch.batch_size)\n",
    "            x_all = torch.cat(xs, dim=0)\n",
    "        pbar.close()\n",
    "        return x_all\n",
    "\n",
    "\n",
    "model = SAGE(dataset.num_features, 256, dataset.num_classes).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "\n",
    "    pbar = tqdm(total=int(len(train_loader.dataset)))\n",
    "    pbar.set_description(f'Epoch {epoch:02d}')\n",
    "\n",
    "    total_loss = total_correct = total_examples = 0\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        print(batch.y.shape, batch.x.shape, batch.batch_size)\n",
    "        y = batch.y[:batch.batch_size]\n",
    "        y_hat = model(batch.x, batch.edge_index.to(device))[:batch.batch_size]\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += float(loss) * batch.batch_size\n",
    "        total_correct += int((y_hat.argmax(dim=-1) == y).sum())\n",
    "        total_examples += batch.batch_size\n",
    "        pbar.update(batch.batch_size)\n",
    "    pbar.close()\n",
    "\n",
    "    return total_loss / total_examples, total_correct / total_examples\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test():\n",
    "    model.eval()\n",
    "    y_hat = model.inference(data.x, subgraph_loader).argmax(dim=-1)\n",
    "    y = data.y.to(y_hat.device)\n",
    "\n",
    "    accs = []\n",
    "    for mask in [data.train_mask, data.val_mask, data.test_mask]:\n",
    "        accs.append(int((y_hat[mask] == y[mask]).sum()) / int(mask.sum()))\n",
    "    return accs\n",
    "\n",
    "\n",
    "times = []\n",
    "for epoch in range(1, 11):\n",
    "    start = time.time()\n",
    "    loss, acc = train(epoch)\n",
    "    print(f'Epoch {epoch:02d}, Loss: {loss:.4f}, Approx. Train: {acc:.4f}')\n",
    "    train_acc, val_acc, test_acc = test()\n",
    "    print(f'Epoch: {epoch:02d}, Train: {train_acc:.4f}, Val: {val_acc:.4f}, '\n",
    "          f'Test: {test_acc:.4f}')\n",
    "    times.append(time.time() - start)\n",
    "print(f\"Median time per epoch: {torch.tensor(times).median():.4f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import NeighborLoader\n",
    "from torch_sparse import SparseTensor\n",
    "\n",
    "\n",
    "x = torch.randn(8, 32)  # Node features of shape [num_nodes, num_features]\n",
    "y = torch.randint(0, 4, (8, ))  # Node labels of shape [num_nodes]\n",
    "edge_index = torch.tensor([\n",
    "    [2, 3, 3, 4, 5, 6, 7],\n",
    "    [0, 0, 1, 1, 2, 3, 4]],\n",
    ")\n",
    "edge_feature = torch.randn(edge_index.shape[1], 16)\n",
    "adj_t = SparseTensor.from_edge_index(edge_index, edge_feature, sparse_sizes=(data.num_nodes, data.num_nodes)).t()\n",
    "\n",
    "#   0  1\n",
    "#  / \\/ \\\n",
    "# 2  3  4\n",
    "# |  |  |\n",
    "# 5  6  7\n",
    "\n",
    "data = Data(x=x, y=y, adj_t=adj_t)\n",
    "\n",
    "loader = NeighborLoader(\n",
    "    data,\n",
    "    input_nodes=torch.tensor([0, 1]),\n",
    "    num_neighbors=[2, 1],\n",
    "    batch_size=2,\n",
    "    replace=False,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NeighborSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[8, 32], y=[8], adj_t=[8, 8, 16, nnz=7], n_id=[8], e_id=[7], num_sampled_nodes=[3], num_sampled_edges=[2], input_id=[2], batch_size=2)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4, 5, 6, 7])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.n_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 3, 0, 2, 0])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
